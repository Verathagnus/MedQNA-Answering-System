# MedQNA-Answering-System

This repository contains a **Medical Question Answering System** built using LangChain, the LLaMA 3.1 model, and ChromaDB. Designed as a retrieval-augmented generation (RAG) system, it answers medical questions in English by retrieving relevant documents and generating concise responses. The Streamlit interface is included in this repository.

## Features

- **RAG Architecture**: Combines document retrieval with LLM generation for contextually accurate answers.
- **ChromaDB for Fast Retrieval**: Uses ChromaDB to store embeddings and retrieve documents that match user queries.
- **LLaMA 3.1 with Ollama Integration**: Utilizes LLaMA 3.1 8B for both embedding generation and answer synthesis.
- **Interactive Streamlit Interface**: Simple UI for users to ask questions, view answers, and display retrieved context.

## Setup and Requirements

### Step 1: Clone this Repository

```sh
git clone https://github.com/yourusername/MedQNA-Answering-System.git
cd MedQNA-Answering-System
```

### Step 2: Download and Set Up the ChromaDB Vector Database

1. Download the pre-built ChromaDB vector files from [Google Drive](https://drive.google.com/file/d/1q9jWfLbHzLkXOqCAvr_RMH8cFeHZOzK4/view?usp=drivesdk).
2. Extract the files and update the `persist_root` variable in `streamlit_app.py` to point to the directory where the vector database was extracted.

   ```python
   persist_root = "/path/to/extracted/vectordb"
   ```

### Step 3: Install Ollama and Prepare the LLaMA Model

1. Download and install Ollama from [ollama.com/download](https://ollama.com/download).
2. Start the Ollama instance:

   ```sh
   ollama serve
   ollama pull llama3.1:8b
   ```

### Step 4: Install Project Dependencies

```sh
pip install -r requirements.txt
```

### Step 5: Run the Streamlit Application

```sh
streamlit run streamlit_app.py
```

## Workflow

### 1. **User Query Input**

   - The user inputs a medical question in the Streamlit text box.
   - This question is passed to the vector database for document retrieval.

### 2. **Document Retrieval**

   - The ChromaDB retriever searches for the top `k` relevant documents (default `k=20`).
   - It uses embeddings generated by `OllamaEmbeddings` (LLaMA 3.1 model) to identify documents matching the query context.

### 3. **RAG Prompt Creation**

   - The retrieved documents are formatted and combined into a prompt template specifically designed for retrieval-augmented generation (RAG).
   - This prompt includes the question and retrieved context, guiding the LLaMA model to generate an informed answer.

### 4. **Answer Generation**

   - The RAG prompt is sent to the `ChatOllama` model to generate a concise answer (limited to three sentences).
   - The answer is returned and displayed in the Streamlit interface.

### 5. **Display Results**

   - The final answer is displayed to the user.
   - If enabled, the “Show Retrieved Context” option displays the documents used to generate the answer, providing transparency on the source of information.

### Streamlit Workflow Management

   - `st.session_state` is used to manage the processing, answer, and error states, ensuring responsive interactions.
   - A loading spinner provides feedback during processing, making it clear when the system is working to generate an answer.

## Usage

- **Question Input**: Type a medical question to retrieve relevant documents and view an answer.
- **Context Display**: Enable “Show Retrieved Context” to see the documents contributing to the answer.

---
